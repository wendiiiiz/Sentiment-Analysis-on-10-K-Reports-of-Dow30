{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime, timedelta\n",
    "import unicodedata\n",
    "\n",
    "# Importing libraries you need to install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from lxml import html\n",
    "from tqdm import tqdm\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"/Users/WendiZhang/Desktop/FRE_NLP/Project/cik2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "for i in range(len(sample)):\n",
    "    c.append(re.findall(r\"\\d{10}\",sample['cik'][i] ))\n",
    "cik = []\n",
    "for i in range(len(c)):\n",
    "    cik += c[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['cik'] = cik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>American Express Co</td>\n",
       "      <td>0000004962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Amgen Inc</td>\n",
       "      <td>0000318154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>0000320193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Boeing Co</td>\n",
       "      <td>0000012927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Caterpillar Inc</td>\n",
       "      <td>0000018230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Cisco Systems Inc</td>\n",
       "      <td>0000858877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Chevron Corp</td>\n",
       "      <td>0000093410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Goldman Sachs Group Inc</td>\n",
       "      <td>0000886982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Home Depot Inc</td>\n",
       "      <td>0000354950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Honeywell International Inc</td>\n",
       "      <td>0000773840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>International Business Machines Corp</td>\n",
       "      <td>0000051143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Intel Corp</td>\n",
       "      <td>0000050863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Johnson &amp; Johnson</td>\n",
       "      <td>0000200406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Coca-Cola Co</td>\n",
       "      <td>0000317540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>JPMorgan Chase &amp; Co</td>\n",
       "      <td>0000019617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>McDonald’s Corp</td>\n",
       "      <td>0000063908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3M Co</td>\n",
       "      <td>0000066740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Merck &amp; Co Inc</td>\n",
       "      <td>0000064978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Microsoft Corp</td>\n",
       "      <td>0000789019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Nike Inc</td>\n",
       "      <td>0000320187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Procter &amp; Gamble Co</td>\n",
       "      <td>0000080424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Travelers Companies Inc</td>\n",
       "      <td>0000086312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>UnitedHealth Group Inc</td>\n",
       "      <td>0000731766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Salesforce.Com Inc</td>\n",
       "      <td>0001108524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Verizon Communications Inc</td>\n",
       "      <td>0000732712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Visa Inc</td>\n",
       "      <td>0001403161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Walgreens Boots Alliance Inc</td>\n",
       "      <td>0001618921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Walmart Inc</td>\n",
       "      <td>0000104169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Walt Disney Co</td>\n",
       "      <td>0001744489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Dow Inc</td>\n",
       "      <td>0001751788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>Kraft</td>\n",
       "      <td>0001103982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>Alcoa</td>\n",
       "      <td>0001675149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>Bank of America</td>\n",
       "      <td>0000070858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>Hewlett-Packard</td>\n",
       "      <td>0001645590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>0000732717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>General Electric</td>\n",
       "      <td>0000040545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>DowDuPont</td>\n",
       "      <td>0001666700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>ExxonMobil</td>\n",
       "      <td>0000034088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>Raytheon</td>\n",
       "      <td>0000101829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>Pfizer</td>\n",
       "      <td>0000078003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Name         cik\n",
       "0                    American Express Co  0000004962\n",
       "1                              Amgen Inc  0000318154\n",
       "2                              Apple Inc  0000320193\n",
       "3                              Boeing Co  0000012927\n",
       "4                        Caterpillar Inc  0000018230\n",
       "5                      Cisco Systems Inc  0000858877\n",
       "6                           Chevron Corp  0000093410\n",
       "7                Goldman Sachs Group Inc  0000886982\n",
       "8                         Home Depot Inc  0000354950\n",
       "9            Honeywell International Inc  0000773840\n",
       "10  International Business Machines Corp  0000051143\n",
       "11                            Intel Corp  0000050863\n",
       "12                     Johnson & Johnson  0000200406\n",
       "13                          Coca-Cola Co  0000317540\n",
       "14                   JPMorgan Chase & Co  0000019617\n",
       "15                       McDonald’s Corp  0000063908\n",
       "16                                 3M Co  0000066740\n",
       "17                        Merck & Co Inc  0000064978\n",
       "18                        Microsoft Corp  0000789019\n",
       "19                              Nike Inc  0000320187\n",
       "20                   Procter & Gamble Co  0000080424\n",
       "21               Travelers Companies Inc  0000086312\n",
       "22                UnitedHealth Group Inc  0000731766\n",
       "23                    Salesforce.Com Inc  0001108524\n",
       "24            Verizon Communications Inc  0000732712\n",
       "25                              Visa Inc  0001403161\n",
       "26          Walgreens Boots Alliance Inc  0001618921\n",
       "27                           Walmart Inc  0000104169\n",
       "28                        Walt Disney Co  0001744489\n",
       "29                               Dow Inc  0001751788\n",
       "30                                 Kraft  0001103982\n",
       "31                                 Alcoa  0001675149\n",
       "32                       Bank of America  0000070858\n",
       "33                       Hewlett-Packard  0001645590\n",
       "34                                  AT&T  0000732717\n",
       "35                      General Electric  0000040545\n",
       "36                             DowDuPont  0001666700\n",
       "37                            ExxonMobil  0000034088\n",
       "38                              Raytheon  0000101829\n",
       "39                                Pfizer  0000078003"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WriteLogFile(log_file_name, text):\n",
    "    '''\n",
    "    Helper function.\n",
    "    Writes a log file with all notes and\n",
    "    error messages from a scraping \"session\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "    text : str\n",
    "        Text to write to the log file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    with open(log_file_name, \"a\") as log_file:\n",
    "        log_file.write(text)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape10K(browse_url_base, filing_url_base, doc_url_base, cik, log_file_name):\n",
    "    \n",
    "    '''\n",
    "    Scrapes all 10-Ks and 10-K405s for a particular \n",
    "    CIK from EDGAR.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    browse_url_base : str\n",
    "        Base URL for browsing EDGAR.\n",
    "    filing_url_base : str\n",
    "        Base URL for filings listings on EDGAR.\n",
    "    doc_url_base : str\n",
    "        Base URL for one filing's document tables\n",
    "        page on EDGAR.\n",
    "    cik : str\n",
    "        Central Index Key.\n",
    "    log_file_name : str\n",
    "        Name of the log file (should be a .txt file).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Check if we've already scraped this CIK\n",
    "    try:\n",
    "        os.mkdir(cik)\n",
    "    except OSError:\n",
    "        print(\"Already scraped CIK\", cik)\n",
    "        return\n",
    "    \n",
    "    # If we haven't, go into the directory for that CIK\n",
    "    os.chdir(cik)\n",
    "    \n",
    "    #I avoid print here\n",
    "    \"\"\"\n",
    "    print('Scraping CIK', cik)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Request list of 10-K filings\n",
    "    res = requests.get(browse_url_base % cik)\n",
    "    \n",
    "    # If the request failed, log the failure and exit\n",
    "    if res.status_code != 200:\n",
    "        os.chdir('..')\n",
    "        os.rmdir(cik) # remove empty dir\n",
    "        text = \"Request failed with error code \" + str(res.status_code) + \\\n",
    "               \"\\nFailed URL: \" + (browse_url_base % cik) + '\\n'\n",
    "        WriteLogFile(log_file_name, text)\n",
    "        return\n",
    "\n",
    "    # If the request doesn't fail, continue...\n",
    "    \n",
    "    # Parse the response HTML using BeautifulSoup\n",
    "    soup = bs.BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "    # Extract all tables from the response\n",
    "    html_tables = soup.find_all('table')\n",
    "    \n",
    "    # Check that the table we're looking for exists\n",
    "    # If it doesn't, exit\n",
    "    if len(html_tables)<3:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Parse the Filings table\n",
    "    filings_table = pd.read_html(str(html_tables[2]), header=0)[0]\n",
    "    filings_table['Filings'] = [str(x) for x in filings_table['Filings']]\n",
    "\n",
    "    # Get only 10-K and 10-K405 document filings\n",
    "    filings_table = filings_table[(filings_table['Filings'] == '10-K') | (filings_table['Filings'] == '10-K405')]\n",
    "\n",
    "    # If filings table doesn't have any\n",
    "    # 10-Ks or 10-K405s, exit\n",
    "    if len(filings_table)==0:\n",
    "        os.chdir('..')\n",
    "        return\n",
    "    \n",
    "    # Get accession number for each 10-K and 10-K405 filing\n",
    "    filings_table['Acc_No'] = [x.replace('\\xa0',' ')\n",
    "                               .split('Acc-no: ')[1]\n",
    "                               .split(' ')[0] for x in filings_table['Description']]\n",
    "\n",
    "    # Iterate through each filing and \n",
    "    # scrape the corresponding document...\n",
    "    for index, row in filings_table.iterrows():\n",
    "        \n",
    "        # Get the accession number for the filing\n",
    "        acc_no = str(row['Acc_No'])\n",
    "        \n",
    "        # Navigate to the page for the filing\n",
    "        docs_page = requests.get(filing_url_base % (cik, acc_no))\n",
    "        \n",
    "        # If request fails, log the failure\n",
    "        # and skip to the next filing\n",
    "        if docs_page.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(docs_page.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (filing_url_base % (cik, acc_no)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "\n",
    "        # If request succeeds, keep going...\n",
    "        \n",
    "        # Parse the table of documents for the filing\n",
    "        docs_page_soup = bs.BeautifulSoup(docs_page.text, 'lxml')\n",
    "        docs_html_tables = docs_page_soup.find_all('table')\n",
    "        if len(docs_html_tables)==0:\n",
    "            continue\n",
    "        docs_table = pd.read_html(str(docs_html_tables[0]), header=0)[0]\n",
    "        docs_table['Type'] = [str(x) for x in docs_table['Type']]\n",
    "        \n",
    "        # Get the 10-K and 10-K405 entries for the filing\n",
    "        docs_table = docs_table[(docs_table['Type'] == '10-K') | (docs_table['Type'] == '10-K405')]\n",
    "        \n",
    "        # If there aren't any 10-K or 10-K405 entries,\n",
    "        # skip to the next filing\n",
    "        if len(docs_table)==0:\n",
    "            continue\n",
    "        # If there are 10-K or 10-K405 entries,\n",
    "        # grab the first document\n",
    "        elif len(docs_table)>0:\n",
    "            docs_table = docs_table.iloc[0]\n",
    "        \n",
    "        docname = docs_table['Document']\n",
    "        \n",
    "        # If that first entry is unavailable,\n",
    "        # log the failure and exit\n",
    "        if str(docname) == 'nan':\n",
    "            os.chdir('..')\n",
    "            text = 'File with CIK: %s and Acc_No: %s is unavailable' % (cik, acc_no) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue       \n",
    "        \n",
    "        # If it is available, continue...\n",
    "        \n",
    "        # Request the file\n",
    "        file = requests.get(doc_url_base % (cik, acc_no.replace('-', ''), docname))\n",
    "        \n",
    "        # If the request fails, log the failure and exit\n",
    "        if file.status_code != 200:\n",
    "            os.chdir('..')\n",
    "            text = \"Request failed with error code \" + str(file.status_code) + \\\n",
    "                   \"\\nFailed URL: \" + (doc_url_base % (cik, acc_no.replace('-', ''), docname)) + '\\n'\n",
    "            WriteLogFile(log_file_name, text)\n",
    "            os.chdir(cik)\n",
    "            continue\n",
    "        \n",
    "        # If it succeeds, keep going...\n",
    "        \n",
    "        # Save the file in appropriate format\n",
    "        \"\"\"\n",
    "        date = str(row['Filing Date'])\n",
    "        filename = cik + '_' + date + '.txt'\n",
    "        html_file = open(filename, 'a')\n",
    "        html_file.write(file.text)\n",
    "        html_file.close()\n",
    "        \"\"\"\n",
    "        if '.txt' in docname:\n",
    "            # Save text as TXT\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.txt'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        else:\n",
    "            # Save text as HTML\n",
    "            date = str(row['Filing Date'])\n",
    "            filename = cik + '_' + date + '.html'\n",
    "            html_file = open(filename, 'a')\n",
    "            html_file.write(file.text)\n",
    "            html_file.close()\n",
    "        \n",
    "    # Move back to the main 10-K directory\n",
    "    os.chdir('..')\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname_10k = '/Users/WendiZhang/Desktop/Scrape_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(sample[\"cik\"])[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0000004962'], dtype=object)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the function to scrape 10-Ks\n",
    "\n",
    "# Define parameters\n",
    "browse_url_base_10k = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=%s&type=10-K'\n",
    "filing_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s-index.html'\n",
    "doc_url_base_10k = 'http://www.sec.gov/Archives/edgar/data/%s/%s/%s'\n",
    "\n",
    "# Set correct directory\n",
    "try:\n",
    "    os.chdir(pathname_10k)\n",
    "except Exception:\n",
    "    os.makedirs(pathname_10k)\n",
    "    os.chdir(pathname_10k)\n",
    "\n",
    "# Initialize log file\n",
    "# (log file name = the time we initiate scraping session)\n",
    "time = strftime(\"%Y-%m-%d %Hh%Mm%Ss\", gmtime())\n",
    "log_file_name = 'log '+time+'.txt'\n",
    "with open(log_file_name, 'a') as log_file:\n",
    "    log_file.close()\n",
    "\n",
    "# Iterate over CIKs and scrape 10-Ks\n",
    "for cik in tqdm(a):\n",
    "    Scrape10K(browse_url_base=browse_url_base_10k, \n",
    "          filing_url_base=filing_url_base_10k, \n",
    "          doc_url_base=doc_url_base_10k, \n",
    "          cik=cik,\n",
    "          log_file_name=log_file_name)\n",
    "\n",
    "#return to the main menu\n",
    "os.chdir('..')\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveNumericalTables(soup):\n",
    "    \n",
    "    '''\n",
    "    Removes tables with >15% numerical characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup\n",
    "        with numerical tables removed.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Determines percentage of numerical characters\n",
    "    # in a table\n",
    "    def GetDigitPercentage(tablestring):\n",
    "        if len(tablestring)>0.0:\n",
    "            numbers = sum([char.isdigit() for char in tablestring])\n",
    "            length = len(tablestring)\n",
    "            return numbers/length\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    # Evaluates numerical character % for each table\n",
    "    # and removes the table if the percentage is > 15%\n",
    "    [x.extract() for x in soup.find_all('table') if GetDigitPercentage(x.get_text())>0.15]\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveTags(soup):\n",
    "    \n",
    "    '''\n",
    "    Drops HTML tags, newlines and unicode text from\n",
    "    filing text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup : BeautifulSoup object\n",
    "        Parsed result from BeautifulSoup.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        Filing text.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Remove HTML tags with get_text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove newline characters\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    '''\n",
    "    Adding a replace to convert all excecute space to one space \n",
    "    '''\n",
    "    text = re.sub(r'\\s', ' ', text)\n",
    "    \n",
    "    # Replace unicode characters with their\n",
    "    # \"normal\" representations\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertHTML(cik):\n",
    "    \n",
    "    '''\n",
    "    Removes numerical tables, HTML tags,\n",
    "    newlines, unicode text, and XBRL tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cik : str\n",
    "        Central Index Key used to scrape files.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Look for files scraped for that CIK\n",
    "    try: \n",
    "        os.chdir(cik)\n",
    "    # ...if we didn't scrape any files for that CIK, exit\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find directory for CIK\", cik)\n",
    "        return\n",
    "    \"\"\" avoid print\"\"\"    \n",
    "    #print(\"Parsing CIK %s...\" % cik)\n",
    "    \n",
    "    parsed = False # flag to tell if we've parsed anything\n",
    "    \n",
    "    # Try to make a new directory within the CIK directory\n",
    "    # to store the text representations of the filings\n",
    "    try:\n",
    "        os.mkdir('rawtext')\n",
    "    # If it already exists, continue\n",
    "    # We can't exit at this point because we might be\n",
    "    # partially through parsing text files, so we need to continue\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    # Get list of scraped files\n",
    "    # excluding hidden files and directories\n",
    "    file_list = [fname for fname in os.listdir() if not (fname.startswith('.') | os.path.isdir(fname))]\n",
    "    \n",
    "    # Iterate over scraped files and clean\n",
    "    for filename in file_list:\n",
    "            \n",
    "        # Check if file has already been cleaned\n",
    "        new_filename = filename.replace('.html', '.txt')\n",
    "        text_file_list = os.listdir('rawtext')\n",
    "        if new_filename in text_file_list:\n",
    "            continue\n",
    "        \n",
    "        # If it hasn't been cleaned already, keep going...\n",
    "        \n",
    "        # Clean file\n",
    "        with open(filename, 'r',encoding='utf-8') as file:\n",
    "            parsed = True\n",
    "            soup = bs.BeautifulSoup(file.read(), \"lxml\")\n",
    "            soup = RemoveNumericalTables(soup)\n",
    "            \n",
    "            #add my delete reference function here\n",
    "            #soup = delete_reference(soup)\n",
    "            \n",
    "            text = RemoveTags(soup)\n",
    "            with open('rawtext/'+new_filename, 'w',encoding='utf-8') as newfile:\n",
    "                newfile.write(text)\n",
    "    \n",
    "    # If all files in the CIK directory have been parsed\n",
    "    # then log that\n",
    "    if parsed==False:\n",
    "        print(\"Already parsed CIK\", cik)\n",
    "    \n",
    "    os.chdir('..')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# For 10-Ks...\n",
    "# -*- coding: utf-8 -*-\n",
    "os.chdir(pathname_10k)\n",
    "\n",
    "# Iterate over CIKs and clean HTML filings\n",
    "for cik in tqdm(a):\n",
    "    ConvertHTML(cik)\n",
    "os.chdir('..')\n",
    "os.chdir('..')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
